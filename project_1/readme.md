<!--
 * @Description: 
 * @Author: 唐健峰
 * @Date: 2023-09-14 15:21:44
 * @LastEditors: ${author}
 * @LastEditTime: 2023-09-26 21:41:14
-->
# 逻辑回归

## 打包

```bash
pyinstaller src/python/index.py
```

## 运行

```bash
python -u src/python/index.py
```

## 词袋模型
词袋模型(Bag of Words，BoW):将文本视为词汇表中的词语集合，然后计算每个词语在文本中的出现频率。这将创建一个词频向量，其中每个元素表示对应词汇表中词语的出现次数。词袋模型忽略了词语的顺序和语法结构，只关注词汇表中的词汇。

## 我采用的词袋模型的得分算法

$$f(x) = \frac{ TF(t, d) \cdot LDF(t, D)}{ e ^ { \left( -\sum_{i=0}^{N} p(x) \cdot \log_2(p(x)) \right)}}$$

## 线性得分函数成功实施下输出有至少90%的成功率(w未对参数b=0进行优化)

**该得分算法具有良好的区分性**

```python
python -u python /Volumes/TJF_YINGPAN/ai_project/project_1/src/python/__test__.py
```

```bash

tangjianfeng@tangjianfengdeMacBook-Air project_1 % python -u /Volumes/TJF_YINGPAN/ai_project/project_1/src/python/__test__.py
/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/cloud/duringbug/preprocessing/read.py:44: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  random_numbers = set(random.sample(all_numbers, 400))
训练集4000条与测试集4000条划分成功
遍历my_train_txt中: 100%|████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:25<00:00, 158.32it/s]
计算第0类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6547/6547 [00:00<00:00, 2424733.62it/s]
计算第1类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5010/5010 [00:00<00:00, 2459440.90it/s]
计算第2类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5127/5127 [00:00<00:00, 2514228.53it/s]
计算第3类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4236/4236 [00:00<00:00, 2496076.39it/s]
计算第4类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4375/4375 [00:00<00:00, 2474724.21it/s]
计算第5类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5380/5380 [00:00<00:00, 2475900.32it/s]
计算第6类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6558/6558 [00:00<00:00, 2474473.34it/s]
计算第7类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4732/4732 [00:00<00:00, 2599875.10it/s]
计算第8类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4634/4634 [00:00<00:00, 2564846.23it/s]
计算第9类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6335/6335 [00:00<00:00, 2493282.90it/s]
计算每类的词总数: 100%|█████████████████████████████████████████████████████████████████████████████████| 23680/23680 [00:00<00:00, 377437.31it/s]
计算TF中: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 23680/23680 [00:00<00:00, 234290.47it/s]
储存TF-IDF中: 100%|█████████████████████████████████████████████████████████████████████████████████████| 23680/23680 [00:00<00:00, 220600.85it/s]
计算每个词的信息熵: 100%|████████████████████████████████████████████████████████████████████████████████| 23680/23680 [00:00<00:00, 87399.10it/s]
储存score向量矩阵: 100%|████████████████████████████████████████████████████████████████████████████████| 23680/23680 [00:00<00:00, 141672.58it/s]
遍历test_my_train_txt中: 100%|███████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:27<00:00, 144.28it/s]
正确率:0.95775
遍历test_my_verification_data中: 100%|███████████████████████████████████████████████████████████████████████| 4000/4000 [00:28<00:00, 141.58it/s]
正确率:0.92175
.
----------------------------------------------------------------------
Ran 2 tests in 82.571s

OK
```

## 偏置系数提高不大，大概在0.2到0.4个百分点之间


**使用BFGS算法**

```bash
tangjianfeng@tangjianfengdeMacBook-Air project_1 % python -u "/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/__test__.py"
/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/cloud/duringbug/preprocessing/read.py:44: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  random_numbers = set(random.sample(all_numbers, 400))
训练集4000条与测试集4000条划分成功
遍历my_train_txt中: 100%|████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:24<00:00, 160.56it/s]
计算第0类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6559/6559 [00:00<00:00, 2418500.21it/s]
计算第1类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4967/4967 [00:00<00:00, 2526756.58it/s]
计算第2类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5069/5069 [00:00<00:00, 2507184.78it/s]
计算第3类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4208/4208 [00:00<00:00, 2462281.14it/s]
计算第4类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4503/4503 [00:00<00:00, 2364711.52it/s]
计算第5类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5401/5401 [00:00<00:00, 2475244.31it/s]
计算第6类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6430/6430 [00:00<00:00, 2456003.53it/s]
计算第7类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4652/4652 [00:00<00:00, 2553245.51it/s]
计算第8类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4698/4698 [00:00<00:00, 2460951.69it/s]
计算第9类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6396/6396 [00:00<00:00, 2395674.98it/s]
计算每类的词总数: 100%|█████████████████████████████████████████████████████████████████████████████████| 23603/23603 [00:00<00:00, 379656.76it/s]
计算TF中: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 23603/23603 [00:00<00:00, 245742.02it/s]
储存TF-IDF中: 100%|█████████████████████████████████████████████████████████████████████████████████████| 23603/23603 [00:00<00:00, 219744.91it/s]
计算每个词的信息熵: 100%|████████████████████████████████████████████████████████████████████████████████| 23603/23603 [00:00<00:00, 89503.61it/s]
储存score向量矩阵: 100%|████████████████████████████████████████████████████████████████████████████████| 23603/23603 [00:00<00:00, 140608.00it/s]
遍历test_my_train_txt中: 100%|████████████████████████████████████████████████████████████████████████████████| 4000/4000 [05:08<00:00, 12.97it/s]
正确率:0.9605
遍历test_my_verification_data中: 100%|███████████████████████████████████████████████████████████████████████| 4000/4000 [00:27<00:00, 146.43it/s]
[-1.58153280e-05 -1.00945128e-03  1.73159759e-04  1.21804881e-04
  9.75286359e-04  3.00024779e-04 -6.88758915e-04  6.81572796e-04
 -1.83270574e-04 -3.54697815e-04]
正确率:0.925
无偏置向量b下遍历test_my_verification_data中: 100%|██████████████████████████████████████████████████████████| 4000/4000 [00:27<00:00, 146.14it/s]
正确率:0.92275
.
----------------------------------------------------------------------
Ran 1 test in 389.487s

OK
```

# 决策树训练集
True Labels表示真实标签，Predicted Labels下的Score表示相应标签不同分类下的得分

![p_1](https://img-blog.csdnimg.cn/4c8296e95cb44ad8b37b90f61a572736.png#pic_center)
![p_2](https://img-blog.csdnimg.cn/778dfc2910d746929ed8957b4d220890.png#pic_center)
![p_3](https://img-blog.csdnimg.cn/dda5d75d101446b2ae8e3fee0ec69669.png#pic_center)
![p_4](https://img-blog.csdnimg.cn/957cc0ff6843412e9497bae18ccecbce.png#pic_center)

我观测到因为不同类之间有相似性，比如True Labels的第6类，中的第6与9类得分相近，我想到用向量距离化来分类
## 余弦距离
**之所以用余弦距离是因为高纬度(这里是十维)下余弦距离更优**
高维空间中的数据集通常会受到维度灾难的影响。闵可夫斯基距离等基于距离的度量会变得不稳定，因为在高维空间中，点之间的距离会变得非常远或非常接近，难以准确表示相似性。余弦距离在这种情况下更加稳定，因为它主要关注向量之间的夹角而不是绝对距离。
```python
def cos_dis(sample, average, labels):
    c = int(len(sample)/10)
    p = 0
    dis = np.zeros((sample.shape[0], average.shape[0]))
    for i in range(sample.shape[0]):
        # 对于每个平均行 j
        for j in range(average.shape[0]):
            dot_product = np.dot(sample[i], average[j])
            norm_sample = np.linalg.norm(sample[i])
            norm_average = np.linalg.norm(average[j])
            similarity = dot_product / (norm_sample * norm_average)
            dis[i][j] = similarity
    for j in range(10):
        suc = 0
        all = sample.shape[0]/10
        for i in range(c):
            max_value = max(dis[j*c+i])
            second_max_value = max(
                filter(lambda x: x != max_value, dis[j*c+i]), default=None)
            if np.argmax(dis[j*c+i]) == labels[j*c+i]:
                suc += 1
        print(f'第{j}类文余弦正确率:{suc/all}')
        p += suc/all
    print(f'余弦值总成功率:{p/10}')
    print()
    return dis


def average_dis(sample, labels):
    c = int(len(sample)/10)
    all = sample.shape[0]/10
    p = 0
    for j in range(10):
        suc = 0
        for i in range(c):
            if np.argmax(sample[i+j*c]) == labels[i+j*c]:
                suc += 1
        print(f'第{j}类平均值成功率:{suc/all}')
        p += suc/all
    print(f'平均值总成功率:{p/10}')
    print()
```

运行结果
```python
tangjianfeng@tangjianfengdeMacBook-Air project_1 % python -u "/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/__test__.py"

Error deleting file: [Errno 2] No such file or directory: 'out/results.txt'
Error deleting file: [Errno 2] No such file or directory: 'b.txt'
文件 'BoW.db' 删除成功
./Volumes/TJF_YINGPAN/ai_project/project_1/src/python/cloud/duringbug/preprocessing/read.py:48: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  random_numbers = set(random.sample(all_numbers, app_config.TRAIN_NUM))
训练集4000条与测试集4000条划分成功
遍历my_train_txt中: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:24<00:00, 164.19it/s]
计算第0类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 6372/6372 [00:00<00:00, 2407106.65it/s]
计算第1类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 4906/4906 [00:00<00:00, 2517095.46it/s]
计算第2类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 5087/5087 [00:00<00:00, 2476659.83it/s]
计算第3类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 4214/4214 [00:00<00:00, 2393013.41it/s]
计算第4类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 4301/4301 [00:00<00:00, 2341298.05it/s]
计算第5类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 5248/5248 [00:00<00:00, 2030787.65it/s]
计算第6类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 6628/6628 [00:00<00:00, 2303027.66it/s]
计算第7类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 4678/4678 [00:00<00:00, 2445588.20it/s]
计算第8类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 4534/4534 [00:00<00:00, 2519138.21it/s]
计算第9类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 6494/6494 [00:00<00:00, 2262652.45it/s]
计算每类的词总数: 100%|███████████████████████████████████████████████████████████████████████████████████████| 23467/23467 [00:00<00:00, 284531.60it/s]
计算TF中: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 23467/23467 [00:00<00:00, 201159.06it/s]
储存TF-IDF中: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 23467/23467 [00:00<00:00, 208596.97it/s]
计算每个词的信息熵: 100%|██████████████████████████████████████████████████████████████████████████████████████| 23467/23467 [00:00<00:00, 90757.87it/s]
储存score向量矩阵: 100%|██████████████████████████████████████████████████████████████████████████████████████| 23467/23467 [00:00<00:00, 139790.87it/s]
遍历resources/exp1_data/my_train_data.txt中: 100%|█████████████████████████████████████████████████████████████████| 4000/4000 [00:08<00:00, 492.33it/s]
遍历resources/exp1_data/my_verification_data.txt中: 100%|██████████████████████████████████████████████████████████| 4000/4000 [00:08<00:00, 498.62it/s]
第0类文余弦正确率:0.98
第1类文余弦正确率:0.93
第2类文余弦正确率:0.9775
第3类文余弦正确率:0.885
第4类文余弦正确率:0.9325
第5类文余弦正确率:0.95
第6类文余弦正确率:0.9225
第7类文余弦正确率:0.86
第8类文余弦正确率:0.9725
第9类文余弦正确率:0.6625
余弦值总成功率:0.90725

第0类平均值成功率:0.9925
第1类平均值成功率:0.9625
第2类平均值成功率:0.9675
第3类平均值成功率:0.96
第4类平均值成功率:0.9775
第5类平均值成功率:0.96
第6类平均值成功率:0.89
第7类平均值成功率:0.9475
第8类平均值成功率:0.98
第9类平均值成功率:0.975
平均值总成功率:0.9612499999999999
```
发现对于相似的文章用余弦值计算更加准确:
比如上面的 _(第6类文余弦正确率:0.9225)_ $>$ _(第6类平均值成功率:0.89)_

## 根节点
根据上面的实验基础，我打算将每个文本对应不同类中的得分为一个向量
$\left(Score_1,Score_2,\cdots,Score_8,Score_9 \right)$
如果第二个最大值的概率大于一个阀值，说明该段文本存在相似文本，就用余弦算法，如下
```python
class RootNode:
    def __init__(self, true_branch, false_branch):
        self.true_branch = true_branch
        self.false_branch = false_branch
        self.test = 0

    def get_result(self, score_matrix_list, threshold_ratio):
        sorted_list = sorted(score_matrix_list, reverse=True)
        sum = 0
        for i in sorted_list:
            sum += i

        if sorted_list[1]/sum < threshold_ratio:
            return self.true_branch.get_result(score_matrix_list)
        else:
            self.test += 1
            return self.false_branch.get_result(score_matrix_list)


class LeafAverageNode:
    def __init__(self, true_branch, false_branch):
        self.true_branch = true_branch
        self.false_branch = false_branch

    def get_result(self, score_matrix_list):
        return max(enumerate(score_matrix_list), key=lambda x: x[1])[0]


class LeafCosNode:
    def __init__(self, true_branch, false_branch, average_matrix):
        self.true_branch = true_branch
        self.false_branch = false_branch
        self.average_matrix = average_matrix

    def get_result(self, score_matrix_list):
        score_vector = np.array(score_matrix_list)
        cosine_distances = np.dot(self.average_matrix, score_vector) / (
            np.linalg.norm(self.average_matrix, axis=1) * np.linalg.norm(score_vector))
        return np.argmax(cosine_distances)

```
```python
def test_root_node(self):
        before_decision_tree_to_results_txt()
        labels, sample, average, result = processing_json_txt(
            "resources/exp1_data/my_train_data.txt", 5)
        labels_2, sample_2, average_2, result_2 = processing_json_txt(
            "resources/exp1_data/my_verification_data.txt", 5)
        # 创建一个RootNode实例，其中true_branch是LeafAverageNode，false_branch是LeafCosNode
        # threshold_ratio = 0.19
        threshold_ratio = 0.19
        root_node = RootNode(LeafAverageNode(None, None),
                             LeafCosNode(None, None, average))
        suc = 0
        all = sample_2.shape[0]
        for column_index in range(sample_2.shape[0]):
            # pdb.set_trace()
            column_list = sample_2[column_index, :].tolist()
            result = root_node.get_result(column_list, threshold_ratio)
            if result == labels[column_index]:
                suc += 1
        print(
            f'threshold_ratio为{threshold_ratio}时,决策树正确率{suc/all},进行余弦算法的个数{root_node.test}')

        root_node.test = 0
        suc = 0
        all = sample_2.shape[0]
        for column_index in range(sample_2.shape[0]):
            # pdb.set_trace()
            column_list = sample_2[column_index, :].tolist()
            result = root_node.get_result(column_list, 1)
            if result == labels[column_index]:
                suc += 1
        print(
            f'threshold_ratio为{0}时,决策树正确率{suc/all},进行余弦算法的个数{root_node.test}')

```
```bash
tangjianfeng@tangjianfengdeMacBook-Air project_1 % python -u "/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/__test__.py"

Error deleting file: [Errno 2] No such file or directory: 'out/results.txt'
Error deleting file: [Errno 2] No such file or directory: 'b.txt'
文件 'BoW.db' 删除成功
.s所有所需的包都已安装并可用。
./Volumes/TJF_YINGPAN/ai_project/project_1/src/python/cloud/duringbug/preprocessing/read.py:48: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  random_numbers = set(random.sample(all_numbers, app_config.TRAIN_NUM))
训练集4000条与测试集4000条划分成功
遍历my_train_txt中: 100%|████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:23<00:00, 169.66it/s]
计算第0类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6611/6611 [00:00<00:00, 2349080.29it/s]
计算第1类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4926/4926 [00:00<00:00, 2504380.79it/s]
计算第2类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5069/5069 [00:00<00:00, 2477386.04it/s]
计算第3类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4283/4283 [00:00<00:00, 2421051.76it/s]
计算第4类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4453/4453 [00:00<00:00, 2498961.16it/s]
计算第5类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5281/5281 [00:00<00:00, 2479028.47it/s]
计算第6类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6616/6616 [00:00<00:00, 2463120.47it/s]
计算第7类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4656/4656 [00:00<00:00, 2489950.20it/s]
计算第8类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4743/4743 [00:00<00:00, 2439733.12it/s]
计算第9类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6549/6549 [00:00<00:00, 2337346.57it/s]
计算每类的词总数: 100%|█████████████████████████████████████████████████████████████████████████████████| 23665/23665 [00:00<00:00, 379294.00it/s]
计算TF中: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 23665/23665 [00:00<00:00, 245154.62it/s]
储存TF-IDF中: 100%|█████████████████████████████████████████████████████████████████████████████████████| 23665/23665 [00:00<00:00, 226815.24it/s]
计算每个词的信息熵: 100%|████████████████████████████████████████████████████████████████████████████████| 23665/23665 [00:00<00:00, 87949.80it/s]
储存score向量矩阵: 100%|████████████████████████████████████████████████████████████████████████████████| 23665/23665 [00:00<00:00, 141295.23it/s]
遍历resources/exp1_data/my_train_data.txt中: 100%|███████████████████████████████████████████████████████████| 4000/4000 [00:07<00:00, 500.06it/s]
遍历resources/exp1_data/my_verification_data.txt中: 100%|████████████████████████████████████████████████████| 4000/4000 [00:07<00:00, 508.71it/s]
threshold_ratio为0.19时,决策树正确率0.9245,进行余弦算法的个数76
threshold_ratio为0时,决策树正确率0.9225,进行余弦算法的个数0
.
----------------------------------------------------------------------
Ran 4 tests in 43.138s

OK (skipped=1)
```
发现能提高0.2个百分点

# 支持向量机
```python
def support_vector_machine():
    words_train, labels_train = train_txt_to_support_vector_machine_sample(
        "resources/exp1_data/my_train_data.txt")
    words_test, labels_test = train_txt_to_support_vector_machine_sample(
        "resources/exp1_data/my_verification_data.txt")
    # 创建SVM分类器
    svm_classifier = SVC(kernel='linear', C=1)
    print("使用训练集训练SVM模型中...")
    # 使用训练集训练SVM模型
    svm_classifier.fit(words_train, labels_train.reshape(-1))
    print("使用模型对测试集进行预测...")
    # 使用模型对测试集进行预测
    predictions = svm_classifier.predict(words_test)

    # 计算准确度
    accuracy = accuracy_score(labels_test.reshape(-1), predictions)
    print("模型的准确度:", accuracy)
```

```bash
tangjianfeng@tangjianfengdeMacBook-Air project_1 % python -u "/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/__test__.py"

Error deleting file: [Errno 2] No such file or directory: 'out/results.txt'
Error deleting file: [Errno 2] No such file or directory: 'b.txt'
文件 'BoW.db' 删除成功
.s所有所需的包都已安装并可用。
.s/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/cloud/duringbug/preprocessing/read.py:48: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  random_numbers = set(random.sample(all_numbers, app_config.TRAIN_NUM))
训练集4000条与测试集4000条划分成功
遍历my_train_txt中: 100%|████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:24<00:00, 166.37it/s]
计算第0类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6590/6590 [00:00<00:00, 2236283.44it/s]
计算第1类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4899/4899 [00:00<00:00, 2445595.73it/s]
计算第2类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5094/5094 [00:00<00:00, 2443200.07it/s]
计算第3类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4230/4230 [00:00<00:00, 2459371.49it/s]
计算第4类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4348/4348 [00:00<00:00, 2392971.24it/s]
计算第5类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5420/5420 [00:00<00:00, 2478265.31it/s]
计算第6类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6580/6580 [00:00<00:00, 2388241.63it/s]
计算第7类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4663/4663 [00:00<00:00, 2554936.58it/s]
计算第8类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4756/4756 [00:00<00:00, 2512040.02it/s]
计算第9类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6485/6485 [00:00<00:00, 2389323.74it/s]
计算每类的词总数: 100%|█████████████████████████████████████████████████████████████████████████████████| 23729/23729 [00:00<00:00, 385148.62it/s]
计算TF中: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 23729/23729 [00:00<00:00, 252906.73it/s]
储存TF-IDF中: 100%|█████████████████████████████████████████████████████████████████████████████████████| 23729/23729 [00:00<00:00, 217310.87it/s]
计算每个词的信息熵: 100%|████████████████████████████████████████████████████████████████████████████████| 23729/23729 [00:00<00:00, 89120.96it/s]
储存score向量矩阵: 100%|████████████████████████████████████████████████████████████████████████████████| 23729/23729 [00:00<00:00, 138359.81it/s]
遍历resources/exp1_data/my_train_data.txt,数据转成词袋(0,1)向量: 100%|██████████████████████████████████████| 4000/4000 [00:01<00:00, 2698.68it/s]
遍历resources/exp1_data/my_verification_data.txt,数据转成词袋(0,1)向量: 100%|███████████████████████████████| 4000/4000 [00:01<00:00, 2767.66it/s]
使用训练集训练SVM模型中...
使用模型对测试集进行预测...
模型的准确度: 0.90525
.s
----------------------------------------------------------------------
Ran 6 tests in 450.427s

OK (skipped=3)
```
单单的词袋模型就有90%以上的正确率

## 特征向量优化
词汇表: $$V = {w_1, w_2, \ldots, w_M} \text{(训练集中的词汇)}$$ 
词袋向量矩阵: $$D_{N*M} \text{, 大小为 } N_{训练or测试} \times M$$
词频矩阵: $$A_{M*10} ,\text{ 大小为 } M \times 10$$
熵矩阵: $$H_{M*1},\text{ 大小为 } M \times 1$$
满足公式:
$$H_{ij} = -\sum_{j=1}^{10} p(A_{ij}) \cdot \log_2(p(A_{ij}))$$
而我将$D_{N*M}/H_{M*1}^T$作为SVM的特征向量矩阵

**返回的特征矩阵代码如下:**

```python
def train_txt_to_support_vector_machine_sample(path):
    data = readPreprocessing(path)
    json_objects = data.strip().split('\n')
    labels = np.zeros((1, len(json_objects)))
    words_zero_dict = get_words_zero_dict()
    entropy = get_entropy_inBow()
    words = []
    for i, json_str in enumerate(tqdm(json_objects, total=len(json_objects), desc=f'遍历{path},数据转成词袋(0,1)向量')):
        word = words_zero_dict.copy()
        data = json.loads(json_str)
        label = data["label"]
        raw_text = data["raw"]
        labels[0][i] = label
        punctuation_to_split = r'[| -,&!".:?();\n$\'#\*-+]+(?!\s)|\s+'
        target_words = split_text(raw_text, punctuation_to_split)
        for target_word in target_words:
            if target_word in word:
                word[target_word] += 1
        words.append(word)
    return np.array([list(word.values()) for word in words])/entropy, labels

```

**运行结果:**

```bash
tangjianfeng@tangjianfengdeMacBook-Air project_1 % python -u "/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/__test__.py"

Error deleting file: [Errno 2] No such file or directory: 'out/results.txt'
Error deleting file: [Errno 2] No such file or directory: 'b.txt'
文件 'BoW.db' 删除成功
.s所有所需的包都已安装并可用。
.s/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/cloud/duringbug/preprocessing/read.py:48: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  random_numbers = set(random.sample(all_numbers, app_config.TRAIN_NUM))
训练集4000条与测试集4000条划分成功
遍历my_train_txt中: 100%|████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:23<00:00, 167.15it/s]
计算第0类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6389/6389 [00:00<00:00, 2429942.71it/s]
计算第1类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4881/4881 [00:00<00:00, 2482706.50it/s]
计算第2类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5081/5081 [00:00<00:00, 2377427.33it/s]
计算第3类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4296/4296 [00:00<00:00, 2435292.60it/s]
计算第4类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4448/4448 [00:00<00:00, 2425411.36it/s]
计算第5类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5385/5385 [00:00<00:00, 2497658.64it/s]
计算第6类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6484/6484 [00:00<00:00, 2473026.02it/s]
计算第7类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4724/4724 [00:00<00:00, 2548082.83it/s]
计算第8类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4559/4559 [00:00<00:00, 2495345.42it/s]
计算第9类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6463/6463 [00:00<00:00, 2415808.46it/s]
计算每类的词总数: 100%|█████████████████████████████████████████████████████████████████████████████████| 23361/23361 [00:00<00:00, 379501.59it/s]
计算TF中: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 23361/23361 [00:00<00:00, 251675.05it/s]
储存TF-IDF中: 100%|█████████████████████████████████████████████████████████████████████████████████████| 23361/23361 [00:00<00:00, 218899.99it/s]
计算每个词的信息熵: 100%|████████████████████████████████████████████████████████████████████████████████| 23361/23361 [00:00<00:00, 90262.03it/s]
储存score向量矩阵: 100%|████████████████████████████████████████████████████████████████████████████████| 23361/23361 [00:00<00:00, 138817.97it/s]
遍历resources/exp1_data/my_train_data.txt,数据转成词袋(0,1)向量: 100%|██████████████████████████████████████| 4000/4000 [00:01<00:00, 2638.18it/s]
遍历resources/exp1_data/my_verification_data.txt,数据转成词袋(0,1)向量: 100%|███████████████████████████████| 4000/4000 [00:01<00:00, 2711.88it/s]
使用训练集训练SVM模型中...
使用模型对测试集进行预测...
模型的准确度: 0.936
.s
----------------------------------------------------------------------
Ran 6 tests in 395.552s

OK (skipped=3)
```
**发现能增长3个百分点，该特征向量集合具有较好的区分性**

### 发现:
- 1. 逻辑回归和SVM我发现有相同的地方，都最小化损失，不同的是逻辑回归用算出来的最后得分分类，而SVM划分了超平面而不是逻辑回归中求得分最值实现
- 2. 逻辑回归对高维数据不好处理，而SVM对高维数据准确性更高