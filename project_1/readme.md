<!--
 * @Description: 
 * @Author: 唐健峰
 * @Date: 2023-09-14 15:21:44
 * @LastEditors: ${author}
 * @LastEditTime: 2023-09-21 10:53:31
-->
# 逻辑回归

## 打包

```bash
pyinstaller src/python/index.py
```

## 运行

```bash
python -u src/python/index.py
```

## 词袋模型
词袋模型(Bag of Words，BoW):将文本视为词汇表中的词语集合，然后计算每个词语在文本中的出现频率。这将创建一个词频向量，其中每个元素表示对应词汇表中词语的出现次数。词袋模型忽略了词语的顺序和语法结构，只关注词汇表中的词汇。

## 我采用的词袋模型的得分算法

$$f(x) = \frac{ TF(t, d) \cdot LDF(t, D)}{ e ^ { \left( -\sum_{i=0}^{N} p(x) \cdot \log_2(p(x)) \right)}}$$

## 线性得分函数成功实施下输出有至少90%的成功率(w未对参数b=0进行优化)

**该得分算法具有良好的区分性**

```python
python -u python /Volumes/TJF_YINGPAN/ai_project/project_1/src/python/__test__.py
```

```bash

tangjianfeng@tangjianfengdeMacBook-Air project_1 % python -u /Volumes/TJF_YINGPAN/ai_project/project_1/src/python/__test__.py
/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/cloud/duringbug/preprocessing/read.py:44: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  random_numbers = set(random.sample(all_numbers, 400))
训练集4000条与测试集4000条划分成功
遍历my_train_txt中: 100%|████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:25<00:00, 158.32it/s]
计算第0类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6547/6547 [00:00<00:00, 2424733.62it/s]
计算第1类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5010/5010 [00:00<00:00, 2459440.90it/s]
计算第2类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5127/5127 [00:00<00:00, 2514228.53it/s]
计算第3类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4236/4236 [00:00<00:00, 2496076.39it/s]
计算第4类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4375/4375 [00:00<00:00, 2474724.21it/s]
计算第5类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5380/5380 [00:00<00:00, 2475900.32it/s]
计算第6类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6558/6558 [00:00<00:00, 2474473.34it/s]
计算第7类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4732/4732 [00:00<00:00, 2599875.10it/s]
计算第8类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4634/4634 [00:00<00:00, 2564846.23it/s]
计算第9类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6335/6335 [00:00<00:00, 2493282.90it/s]
计算每类的词总数: 100%|█████████████████████████████████████████████████████████████████████████████████| 23680/23680 [00:00<00:00, 377437.31it/s]
计算TF中: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 23680/23680 [00:00<00:00, 234290.47it/s]
储存TF-IDF中: 100%|█████████████████████████████████████████████████████████████████████████████████████| 23680/23680 [00:00<00:00, 220600.85it/s]
计算每个词的信息熵: 100%|████████████████████████████████████████████████████████████████████████████████| 23680/23680 [00:00<00:00, 87399.10it/s]
储存score向量矩阵: 100%|████████████████████████████████████████████████████████████████████████████████| 23680/23680 [00:00<00:00, 141672.58it/s]
遍历test_my_train_txt中: 100%|███████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:27<00:00, 144.28it/s]
正确率:0.95775
遍历test_my_verification_data中: 100%|███████████████████████████████████████████████████████████████████████| 4000/4000 [00:28<00:00, 141.58it/s]
正确率:0.92175
.
----------------------------------------------------------------------
Ran 2 tests in 82.571s

OK
```

## 偏置系数提高不大，大概在0.2到0.4个百分点之间


**使用BFGS算法**

```bash
tangjianfeng@tangjianfengdeMacBook-Air project_1 % python -u "/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/__test__.py"
/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/cloud/duringbug/preprocessing/read.py:44: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  random_numbers = set(random.sample(all_numbers, 400))
训练集4000条与测试集4000条划分成功
遍历my_train_txt中: 100%|████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:24<00:00, 160.56it/s]
计算第0类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6559/6559 [00:00<00:00, 2418500.21it/s]
计算第1类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4967/4967 [00:00<00:00, 2526756.58it/s]
计算第2类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5069/5069 [00:00<00:00, 2507184.78it/s]
计算第3类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4208/4208 [00:00<00:00, 2462281.14it/s]
计算第4类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4503/4503 [00:00<00:00, 2364711.52it/s]
计算第5类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 5401/5401 [00:00<00:00, 2475244.31it/s]
计算第6类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6430/6430 [00:00<00:00, 2456003.53it/s]
计算第7类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4652/4652 [00:00<00:00, 2553245.51it/s]
计算第8类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 4698/4698 [00:00<00:00, 2460951.69it/s]
计算第9类文章的IDF: 100%|████████████████████████████████████████████████████████████████████████████████| 6396/6396 [00:00<00:00, 2395674.98it/s]
计算每类的词总数: 100%|█████████████████████████████████████████████████████████████████████████████████| 23603/23603 [00:00<00:00, 379656.76it/s]
计算TF中: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 23603/23603 [00:00<00:00, 245742.02it/s]
储存TF-IDF中: 100%|█████████████████████████████████████████████████████████████████████████████████████| 23603/23603 [00:00<00:00, 219744.91it/s]
计算每个词的信息熵: 100%|████████████████████████████████████████████████████████████████████████████████| 23603/23603 [00:00<00:00, 89503.61it/s]
储存score向量矩阵: 100%|████████████████████████████████████████████████████████████████████████████████| 23603/23603 [00:00<00:00, 140608.00it/s]
遍历test_my_train_txt中: 100%|████████████████████████████████████████████████████████████████████████████████| 4000/4000 [05:08<00:00, 12.97it/s]
正确率:0.9605
遍历test_my_verification_data中: 100%|███████████████████████████████████████████████████████████████████████| 4000/4000 [00:27<00:00, 146.43it/s]
[-1.58153280e-05 -1.00945128e-03  1.73159759e-04  1.21804881e-04
  9.75286359e-04  3.00024779e-04 -6.88758915e-04  6.81572796e-04
 -1.83270574e-04 -3.54697815e-04]
正确率:0.925
无偏置向量b下遍历test_my_verification_data中: 100%|██████████████████████████████████████████████████████████| 4000/4000 [00:27<00:00, 146.14it/s]
正确率:0.92275
.
----------------------------------------------------------------------
Ran 1 test in 389.487s

OK
```

# 决策树训练集
True Labels表示真实标签，Predicted Labels下的Score表示相应标签不同分类下的得分
![p_1](https://img-blog.csdnimg.cn/4c8296e95cb44ad8b37b90f61a572736.png#pic_center)
![p_2](https://img-blog.csdnimg.cn/778dfc2910d746929ed8957b4d220890.png#pic_center)
![p_3](https://img-blog.csdnimg.cn/dda5d75d101446b2ae8e3fee0ec69669.png#pic_center)
![p_4](https://img-blog.csdnimg.cn/957cc0ff6843412e9497bae18ccecbce.png#pic_center)

我观测到因为不同类之间有相似性，比如True Labels的第6类，中的第6与9类得分相近，我想到用向量距离化来分类
## 余弦距离
**之所以用余弦距离是因为高纬度(这里是十维)下余弦距离更优**
高维空间中的数据集通常会受到维度灾难的影响。闵可夫斯基距离等基于距离的度量会变得不稳定，因为在高维空间中，点之间的距离会变得非常远或非常接近，难以准确表示相似性。余弦距离在这种情况下更加稳定，因为它主要关注向量之间的夹角而不是绝对距离。
```python
def cos_dis(sample, average, labels):
    c = int(len(sample)/10)
    p = 0
    dis = np.zeros((sample.shape[0], average.shape[0]))
    for i in range(sample.shape[0]):
        # 对于每个平均行 j
        for j in range(average.shape[0]):
            dot_product = np.dot(sample[i], average[j])
            norm_sample = np.linalg.norm(sample[i])
            norm_average = np.linalg.norm(average[j])
            similarity = dot_product / (norm_sample * norm_average)
            dis[i][j] = similarity
    for j in range(10):
        suc = 0
        all = sample.shape[0]/10
        for i in range(c):
            max_value = max(dis[j*c+i])
            second_max_value = max(
                filter(lambda x: x != max_value, dis[j*c+i]), default=None)
            if np.argmax(dis[j*c+i]) == labels[j*c+i]:
                suc += 1
        print(f'第{j}类文余弦正确率:{suc/all}')
        p += suc/all
    print(f'余弦值总成功率:{p/10}')
    print()
    return dis


def average_dis(sample, labels):
    c = int(len(sample)/10)
    all = sample.shape[0]/10
    p = 0
    for j in range(10):
        suc = 0
        for i in range(c):
            if np.argmax(sample[i+j*c]) == labels[i+j*c]:
                suc += 1
        print(f'第{j}类平均值成功率:{suc/all}')
        p += suc/all
    print(f'平均值总成功率:{p/10}')
    print()
```

运行结果
```python
tangjianfeng@tangjianfengdeMacBook-Air project_1 % python -u "/Volumes/TJF_YINGPAN/ai_project/project_1/src/python/__test__.py"

Error deleting file: [Errno 2] No such file or directory: 'out/results.txt'
Error deleting file: [Errno 2] No such file or directory: 'b.txt'
文件 'BoW.db' 删除成功
./Volumes/TJF_YINGPAN/ai_project/project_1/src/python/cloud/duringbug/preprocessing/read.py:48: DeprecationWarning: Sampling from a set deprecated
since Python 3.9 and will be removed in a subsequent version.
  random_numbers = set(random.sample(all_numbers, app_config.TRAIN_NUM))
训练集4000条与测试集4000条划分成功
遍历my_train_txt中: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:24<00:00, 164.19it/s]
计算第0类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 6372/6372 [00:00<00:00, 2407106.65it/s]
计算第1类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 4906/4906 [00:00<00:00, 2517095.46it/s]
计算第2类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 5087/5087 [00:00<00:00, 2476659.83it/s]
计算第3类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 4214/4214 [00:00<00:00, 2393013.41it/s]
计算第4类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 4301/4301 [00:00<00:00, 2341298.05it/s]
计算第5类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 5248/5248 [00:00<00:00, 2030787.65it/s]
计算第6类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 6628/6628 [00:00<00:00, 2303027.66it/s]
计算第7类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 4678/4678 [00:00<00:00, 2445588.20it/s]
计算第8类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 4534/4534 [00:00<00:00, 2519138.21it/s]
计算第9类文章的IDF: 100%|██████████████████████████████████████████████████████████████████████████████████████| 6494/6494 [00:00<00:00, 2262652.45it/s]
计算每类的词总数: 100%|███████████████████████████████████████████████████████████████████████████████████████| 23467/23467 [00:00<00:00, 284531.60it/s]
计算TF中: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 23467/23467 [00:00<00:00, 201159.06it/s]
储存TF-IDF中: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 23467/23467 [00:00<00:00, 208596.97it/s]
计算每个词的信息熵: 100%|██████████████████████████████████████████████████████████████████████████████████████| 23467/23467 [00:00<00:00, 90757.87it/s]
储存score向量矩阵: 100%|██████████████████████████████████████████████████████████████████████████████████████| 23467/23467 [00:00<00:00, 139790.87it/s]
遍历resources/exp1_data/my_train_data.txt中: 100%|█████████████████████████████████████████████████████████████████| 4000/4000 [00:08<00:00, 492.33it/s]
遍历resources/exp1_data/my_verification_data.txt中: 100%|██████████████████████████████████████████████████████████| 4000/4000 [00:08<00:00, 498.62it/s]
第0类文余弦正确率:0.98
第1类文余弦正确率:0.93
第2类文余弦正确率:0.9775
第3类文余弦正确率:0.885
第4类文余弦正确率:0.9325
第5类文余弦正确率:0.95
第6类文余弦正确率:0.9225
第7类文余弦正确率:0.86
第8类文余弦正确率:0.9725
第9类文余弦正确率:0.6625
余弦值总成功率:0.90725

第0类平均值成功率:0.9925
第1类平均值成功率:0.9625
第2类平均值成功率:0.9675
第3类平均值成功率:0.96
第4类平均值成功率:0.9775
第5类平均值成功率:0.96
第6类平均值成功率:0.89
第7类平均值成功率:0.9475
第8类平均值成功率:0.98
第9类平均值成功率:0.975
平均值总成功率:0.9612499999999999
```
发现对于相似的文章用余弦值计算更加准确:
比如上面的 _(第6类文余弦正确率:0.9225)_ $>$ _(第6类平均值成功率:0.89)_

## 根节点
根据上面的实验基础，我打算将每个文本对应不同类中的得分为一个向量
$\left(Score_1,Score_2,\cdots,Score_8,Score_9 \right)$